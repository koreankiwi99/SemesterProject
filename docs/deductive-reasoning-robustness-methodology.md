# Investigating the Robustness of Deductive Reasoning with LLMs

**Paper**: Investigating the Robustness of Deductive Reasoning with Large Language Models
**Source**: `pdf/2502.04352.pdf`

## Overview

Framework to test robustness of LLM-based deductive reasoning through two families of perturbations: **adversarial noise** and **counterfactual shifts**.

---

## Perturbation Family 1: Adversarial Noise (Section 3.1)

### Purpose
Test if models maintain correct answers when **irrelevant information** is added

### Formalization
- Original: `f: (c, q) → {true, false}`
- Perturbed: `f': (c', q) → {true, false}` where `c' = d₁...dₖc`
- **Requirement**: `f(c,q) = f'(c',q)` (monotonic reasoning)
- Noise added to **beginning** of context (k ∈ {1, 2, 4} sentences)

---

### Type 1: Encyclopedic (E)

**Characteristics**:
- Real-world factual information
- High formalisation complexity
- Low logical reasoning depth
- Low semantic similarity to reasoning context

**Example**:
> "It is 21 km from Karimnagar, on the highway from Karimnagar to Peddapalli."

**Source**:
- **Automated** - Randomly sampled from **10,000 Wikipedia article abstracts**
- Wikipedia API

**When to use**: Test robustness to out-of-domain facts

---

### Type 2: Logical (L)

**Characteristics**:
- Reasoning-like structure
- Low formalisation complexity (easy to convert to FOL)
- High logical reasoning depth
- High semantic similarity (typical reasoning structure)

**Example**:
> "All dresses are clothes."

**Source**:
- **Automated** - Randomly sampled from **1,001 FOLIO contexts**
- Existing deductive reasoning dataset

**When to use**: Test robustness to in-domain distractors

---

### Type 3: Tautological (T)

**Characteristics**:
- Trivially true statements
- High formalisation complexity (truth constants ⊥, ⊤ not always in FOL)
- Low reasoning depth
- Highest semantic similarity (pure logical structure)
- No referential information

**Example**:
> "True is not false"
> "Not true or false is not true"

**Source**:
- **Manual** - **22 hand-written sentences**
- Use negations + at most one disjunction/conjunction
- Complete list in Appendix B

**When to use**: Test robustness to logical noise with high structural similarity

---

## Perturbation Family 2: Counterfactual Shifts (Section 3.2)

### Purpose
Test if models use **faithful reasoning** by contradicting common-sense knowledge

### Formalization
- Original: `c` with conclusion `true`
- Counterfactual: `c' = neg(c)` with conclusion `false`
- **Requirement**: Premises override world knowledge

### Philosophy
> "If the context states that men are immortal, the reasoning must overwrite its belief of men as mortal."

**Example**:
```
Original:
  "All men are mortal. Socrates is a man."
  Q: Is Socrates mortal?
  A: Yes

Counterfactual:
  "All men are IMMORTAL. Socrates is a man."
  Q: Is Socrates mortal?
  A: No
```

---

### Method

**Step 1: Identify Critical Premise**
- Use backward chaining from question
- Select premise whose negation flips the answer

**Step 2: Apply Predefined Negation Rules**
```
Universal:     ∀x. P(x) → Q(x)  becomes  ∀x. P(x) → ¬Q(x)
Existential:   ∃x. P(x)        becomes  ¬∃x. P(x)
Implication:   P → Q           becomes  P ∧ ¬Q
Conjunction:   P ∧ Q           becomes  P ∧ ¬Q
Disjunction:   P ∨ Q           becomes  ¬P ∧ ¬Q
```

**Step 3: Manual NL Insertion**
- Negation **manually added** to natural language text
- Ensures high data quality
- Preserves grammaticality

**Generation**: **Semi-automated** (rule-based logic + manual NL editing)

---

### Combined Perturbations

Counterfactuals can combine with noise:
- **OC**: Original + Counterfactual
- **EC**: Encyclopedic + Counterfactual
- **LC**: Logical + Counterfactual
- **TC**: Tautological + Counterfactual

Total: **7 perturbed datasets** from one original

---

## Experimental Setup (Section 5)

### Dataset
- **LogicBench (Eval)**: 520 samples, 180 unique contexts
- Covers 9 inference rules (modus ponens, constructive dilemma, etc.)
- Automatically generated by GPT-3.5, manually verified
- 340 negative examples (from negating conclusions)

### Noise Sampling Details

| Type | Source | Count | Method |
|------|--------|-------|--------|
| Encyclopedic | Wikipedia | 10,000 abstracts | API sampling |
| Logical | FOLIO | 1,001 contexts | Dataset sampling |
| Tautological | Manual | 22 sentences | Hand-written |

**Sampling**: Random selection, added to beginning of context

---

## Methodological Framework Dimensions

### Dimension 1: Reasoning Format

**A. Informal Reasoning**
- Few-shot prompting (3 manually engineered examples)
- Two modes:
  1. **Direct**: Answer Yes/No directly
  2. **CoT**: Generate step-by-step reasoning first
- Two of three in-context examples include distractors
- Explicit instruction: "contexts may contain irrelevant details"

**B. Autoformalisation**
- LLM translates NL → Logical form
- Theorem prover evaluates: `cLF ⊨ qLF`
- Transparent, verifiable reasoning chain

---

### Dimension 2: Formalisation Syntax

Three equivalent FOL syntaxes tested:

**FOL**: Mathematical symbols
```
∀x. drinkWater(x) ⇒ hydrated(x)
```

**R-FOL**: Prolog-style with ? for variables
```
∀?x. drinkWater(?x) ⇒ hydrated(?x)
```

**TPTP**: Theorem prover format
```
fof(a0, axiom, ![X]: drinkWater(X) => hydrated(X)).
```

**Question**: Does syntax choice affect translation performance?

---

### Dimension 3: Error Recovery Mechanism

**No Feedback**: Single attempt

**Syntax Feedback**: Error messages from parser
- Reduces syntax errors
- From prior work (Logic-LM++)

**Warning Feedback**: Semantic heuristics + syntax errors
- "Soft critics" to recognize semantic issues
- Novel for logical deduction (from LLM-modulo)

**Finding**: Detailed feedback reduces syntax errors but **doesn't improve overall accuracy** (self-correction challenge)

---

## Key Results

### Adversarial Noise Impact
- **Encyclopedic**: Affects autoformalisation more than informal
- **Logical**: Degrades performance across all approaches
- **Tautological**: Minimal impact (easiest to ignore)

### Counterfactual Impact
- **All approaches affected** (informal + autoformalisation)
- Tests faithfulness to premises vs. world knowledge
- Many models fail to override common sense

### Syntax Choice
- Performance varies by syntax (likely due to training data frequency)
- No single "best" syntax

### Error Recovery
- Feedback reduces **syntax errors**
- Does **NOT improve final accuracy**
- Indicates deeper self-correction challenges

---

## Automation Potential

| Component | Generation Method | Automation |
|-----------|-------------------|------------|
| Encyclopedic noise | Wikipedia API | ✅ Fully automated |
| Logical noise | FOLIO sampling | ✅ Fully automated |
| Tautological noise | Manual writing | ⚠️ Can template |
| Counterfactual logic | Predefined rules | ✅ Automated |
| Counterfactual NL | Manual insertion | ⚠️ Needs LLM |
| Base dataset | GPT-3.5 + manual verify | ⚠️ Semi-automated |

---

## Application to Multi-LogiEval / FOLIO

### Direct Applications
1. **Encyclopedic noise**: Add Wikipedia facts about entities
2. **Logical noise**: Sample from FOLIO/Multi-LogiEval corpus
3. **Tautological noise**: Generate from templates
4. **Counterfactual**: Negate critical premise, flip answer

### Challenges
1. Multi-LogiEval uses specific inference rules → need rule-aware negation
2. FOLIO has complex natural language → NL generation harder
3. Verification requires theorem prover (not just answer checking)

### Opportunities
- LogicBench covers same inference rules as Multi-LogiEval
- Can reuse negation rules for similar logical structures
- Automated verification with Lean prover (already in pipeline)
