{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lean vs CoT Analysis\n",
    "\n",
    "Analyzing what fails with Lean vs CoT on legacy results:\n",
    "1. **FOLIO Dataset** (Oct 6, 2025)\n",
    "2. **Multi-LogiEval Dataset** (Oct 16, 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 1: FOLIO Dataset Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load FOLIO legacy results and flatten nested structure\ndef flatten_folio_results(stories_list):\n    \"\"\"Flatten FOLIO nested structure: stories -> results array\"\"\"\n    flattened = []\n    for story in stories_list:\n        story_id = story.get('story_id')\n        premises = story.get('premises', '')\n        for result in story.get('results', []):\n            flattened.append({\n                'story_id': story_id,\n                'premises': premises,\n                'example_id': result.get('example_id'),\n                'question_num': result.get('question_num'),\n                'conclusion': result.get('conclusion', ''),\n                'ground_truth': result.get('ground_truth'),\n                'prediction': result.get('prediction'),\n                'correct': result.get('correct')\n            })\n    return flattened\n\nwith open('results/legacy/cot_folio_responses_20251006_074511.json') as f:\n    folio_cot_raw = json.load(f)\n    folio_cot = flatten_folio_results(folio_cot_raw)\n\nwith open('results/legacy/lean_folio_responses_20251006_074856.json') as f:\n    folio_lean_raw = json.load(f)\n    folio_lean = flatten_folio_results(folio_lean_raw)\n\nwith open('results/legacy/leaninteract_folio_results_20251006_095029.json') as f:\n    folio_lean_int_raw = json.load(f)\n    folio_lean_int = flatten_folio_results(folio_lean_int_raw)\n\nprint(f\"Loaded FOLIO results (flattened):\")\nprint(f\"  CoT: {len(folio_cot)} questions\")\nprint(f\"  Lean: {len(folio_lean)} questions\")\nprint(f\"  Lean Interactive: {len(folio_lean_int)} questions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Align FOLIO results by example_id\ndef align_folio(cot_list, lean_list):\n    # Index by example_id (unique identifier for each question)\n    cot_dict = {r['example_id']: r for r in cot_list if r.get('example_id')}\n    lean_dict = {r['example_id']: r for r in lean_list if r.get('example_id')}\n    \n    common_ids = set(cot_dict.keys()) & set(lean_dict.keys())\n    \n    aligned = []\n    for qid in sorted(common_ids):\n        aligned.append({\n            'example_id': qid,\n            'cot': cot_dict[qid],\n            'lean': lean_dict[qid]\n        })\n    \n    return aligned\n\nfolio_aligned = align_folio(folio_cot, folio_lean)\nprint(f\"\\n✓ Aligned {len(folio_aligned)} FOLIO questions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate accuracy with proper normalization\ndef normalize_answer(value):\n    \"\"\"Normalize various answer formats\"\"\"\n    if not value:\n        return None\n    val = str(value).lower().strip()\n    # Handle FOLIO format: True/False/Uncertain\n    if val in ['true', 't', 'yes', 'y']:\n        return 'true'\n    elif val in ['false', 'f', 'no', 'n']:\n        return 'false'\n    elif val in ['uncertain', 'unknown', 'u']:\n        return 'uncertain'\n    return val\n\ndef get_accuracy(results):\n    if not results:\n        return 0.0\n    correct = 0\n    total = 0\n    for r in results:\n        # Check different field names\n        gt = normalize_answer(r.get('ground_truth', r.get('label', r.get('answer', ''))))\n        pred = normalize_answer(r.get('prediction', r.get('predicted_label', r.get('model_answer', ''))))\n        \n        if gt and pred:\n            total += 1\n            if gt == pred:\n                correct += 1\n    \n    return (correct / total * 100) if total > 0 else 0.0\n\nfolio_cot_acc = get_accuracy(folio_cot)\nfolio_lean_acc = get_accuracy(folio_lean)\nfolio_lean_int_acc = get_accuracy(folio_lean_int)\n\nprint(\"=\"*70)\nprint(\"FOLIO ACCURACY\")\nprint(\"=\"*70)\nprint(f\"CoT:               {folio_cot_acc:>6.2f}%\")\nprint(f\"Lean:              {folio_lean_acc:>6.2f}%\")\nprint(f\"Lean Interactive:  {folio_lean_int_acc:>6.2f}%\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Categorize FOLIO performance patterns\ndef is_correct(result):\n    gt = normalize_answer(result.get('ground_truth', result.get('label', result.get('answer', ''))))\n    pred = normalize_answer(result.get('prediction', result.get('predicted_label', result.get('model_answer', ''))))\n    return gt == pred if (gt and pred) else None\n\nfolio_both_correct = []\nfolio_both_wrong = []\nfolio_cot_only = []  # CoT correct, Lean wrong\nfolio_lean_only = []  # Lean correct, CoT wrong\n\nfor item in folio_aligned:\n    cot_ok = is_correct(item['cot'])\n    lean_ok = is_correct(item['lean'])\n    \n    if cot_ok is None or lean_ok is None:\n        continue\n    \n    if cot_ok and lean_ok:\n        folio_both_correct.append(item)\n    elif not cot_ok and not lean_ok:\n        folio_both_wrong.append(item)\n    elif cot_ok and not lean_ok:\n        folio_cot_only.append(item)\n    elif not cot_ok and lean_ok:\n        folio_lean_only.append(item)\n\nprint(\"FOLIO PERFORMANCE PATTERNS\")\nprint(\"=\"*70)\nprint(f\"Both correct:          {len(folio_both_correct):>4}\")\nprint(f\"Both wrong:            {len(folio_both_wrong):>4}\")\nprint(f\"CoT ✓, Lean ✗:         {len(folio_cot_only):>4} ← CoT succeeds where Lean fails\")\nprint(f\"Lean ✓, CoT ✗:         {len(folio_lean_only):>4} ← Lean succeeds where CoT fails\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show FOLIO examples where CoT succeeds but Lean fails\nprint(\"\\n\" + \"=\"*70)\nprint(f\"FOLIO: {len(folio_cot_only)} QUESTIONS WHERE COT SUCCEEDS BUT LEAN FAILS\")\nprint(\"=\"*70)\n\nfor i, item in enumerate(folio_cot_only[:5], 1):\n    cot_r = item['cot']\n    lean_r = item['lean']\n    \n    print(f\"\\nExample {i}:\")\n    print(f\"  Example ID: {cot_r.get('example_id')}\")\n    print(f\"  Story ID: {cot_r.get('story_id')}\")\n    print(f\"  Premises: {cot_r.get('premises', '')[:150]}...\")\n    print(f\"  Conclusion: {cot_r.get('conclusion', '')[:150]}...\")\n    \n    print(f\"  Ground Truth: {cot_r.get('ground_truth')}\")\n    print(f\"  CoT:  {cot_r.get('prediction')} ✓\")\n    print(f\"  Lean: {lean_r.get('prediction')} ✗\")\n\nif len(folio_cot_only) > 5:\n    print(f\"\\n... and {len(folio_cot_only) - 5} more\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show FOLIO examples where Lean succeeds but CoT fails\nprint(\"\\n\" + \"=\"*70)\nprint(f\"FOLIO: {len(folio_lean_only)} QUESTIONS WHERE LEAN SUCCEEDS BUT COT FAILS\")\nprint(\"=\"*70)\n\nfor i, item in enumerate(folio_lean_only[:5], 1):\n    cot_r = item['cot']\n    lean_r = item['lean']\n    \n    print(f\"\\nExample {i}:\")\n    print(f\"  Example ID: {cot_r.get('example_id')}\")\n    print(f\"  Story ID: {cot_r.get('story_id')}\")\n    print(f\"  Premises: {cot_r.get('premises', '')[:150]}...\")\n    print(f\"  Conclusion: {cot_r.get('conclusion', '')[:150]}...\")\n    \n    print(f\"  Ground Truth: {cot_r.get('ground_truth')}\")\n    print(f\"  CoT:  {cot_r.get('prediction')} ✗\")\n    print(f\"  Lean: {lean_r.get('prediction')} ✓\")\n\nif len(folio_lean_only) > 5:\n    print(f\"\\n... and {len(folio_lean_only) - 5} more\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FOLIO results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(['CoT', 'Lean', 'Lean Interactive'], \n",
    "        [folio_cot_acc, folio_lean_acc, folio_lean_int_acc],\n",
    "        color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('FOLIO: Accuracy Comparison', fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "\n",
    "# Performance patterns\n",
    "labels = ['Both\\nCorrect', 'Both\\nWrong', 'CoT Only', 'Lean Only']\n",
    "sizes = [len(folio_both_correct), len(folio_both_wrong), \n",
    "         len(folio_cot_only), len(folio_lean_only)]\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12']\n",
    "\n",
    "ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', startangle=90)\n",
    "ax2.set_title('FOLIO: Performance Patterns', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 2: Multi-LogiEval Dataset Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Multi-LogiEval legacy results\n",
    "with open('results/legacy/multilogieval_zero_shot_cot_20251016_115317/all_responses.json') as f:\n",
    "    mlogi_cot = json.load(f)\n",
    "\n",
    "with open('results/legacy/multilogieval_lean_lean_test_20251016_111656/all_results.json') as f:\n",
    "    mlogi_lean = json.load(f)\n",
    "\n",
    "print(f\"Loaded Multi-LogiEval results:\")\n",
    "print(f\"  CoT: {len(mlogi_cot)} questions\")\n",
    "print(f\"  Lean: {len(mlogi_lean)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align Multi-LogiEval results by context + question\n",
    "def align_mlogi(cot_list, lean_list):\n",
    "    # Index by context + question\n",
    "    cot_dict = {}\n",
    "    for r in cot_list:\n",
    "        key = f\"{r.get('logic_type', '')}_{r.get('depth', '')}_{r.get('context', '')[:50]}\"\n",
    "        cot_dict[key] = r\n",
    "    \n",
    "    lean_dict = {}\n",
    "    for r in lean_list:\n",
    "        key = f\"{r.get('logic_type', '')}_{r.get('depth', '')}_{r.get('context', '')[:50]}\"\n",
    "        lean_dict[key] = r\n",
    "    \n",
    "    common_ids = set(cot_dict.keys()) & set(lean_dict.keys())\n",
    "    \n",
    "    aligned = []\n",
    "    for qid in common_ids:\n",
    "        aligned.append({\n",
    "            'question_id': qid,\n",
    "            'cot': cot_dict[qid],\n",
    "            'lean': lean_dict[qid]\n",
    "        })\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "mlogi_aligned = align_mlogi(mlogi_cot, mlogi_lean)\n",
    "print(f\"\\n✓ Aligned {len(mlogi_aligned)} Multi-LogiEval questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Multi-LogiEval accuracy\n",
    "mlogi_cot_acc = get_accuracy(mlogi_cot)\n",
    "mlogi_lean_acc = get_accuracy(mlogi_lean)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MULTI-LOGIEVAL ACCURACY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"CoT:   {mlogi_cot_acc:>6.2f}%\")\n",
    "print(f\"Lean:  {mlogi_lean_acc:>6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize Multi-LogiEval performance patterns\n",
    "mlogi_both_correct = []\n",
    "mlogi_both_wrong = []\n",
    "mlogi_cot_only = []  # CoT correct, Lean wrong\n",
    "mlogi_lean_only = []  # Lean correct, CoT wrong\n",
    "\n",
    "for item in mlogi_aligned:\n",
    "    cot_ok = is_correct(item['cot'])\n",
    "    lean_ok = is_correct(item['lean'])\n",
    "    \n",
    "    if cot_ok is None or lean_ok is None:\n",
    "        continue\n",
    "    \n",
    "    if cot_ok and lean_ok:\n",
    "        mlogi_both_correct.append(item)\n",
    "    elif not cot_ok and not lean_ok:\n",
    "        mlogi_both_wrong.append(item)\n",
    "    elif cot_ok and not lean_ok:\n",
    "        mlogi_cot_only.append(item)\n",
    "    elif not cot_ok and lean_ok:\n",
    "        mlogi_lean_only.append(item)\n",
    "\n",
    "print(\"MULTI-LOGIEVAL PERFORMANCE PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Both correct:          {len(mlogi_both_correct):>4}\")\n",
    "print(f\"Both wrong:            {len(mlogi_both_wrong):>4}\")\n",
    "print(f\"CoT ✓, Lean ✗:         {len(mlogi_cot_only):>4} ← CoT succeeds where Lean fails\")\n",
    "print(f\"Lean ✓, CoT ✗:         {len(mlogi_lean_only):>4} ← Lean succeeds where CoT fails\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Multi-LogiEval examples where CoT succeeds but Lean fails\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"MULTI-LOGIEVAL: {len(mlogi_cot_only)} QUESTIONS WHERE COT SUCCEEDS BUT LEAN FAILS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, item in enumerate(mlogi_cot_only[:5], 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    cot_r = item['cot']\n",
    "    lean_r = item['lean']\n",
    "    \n",
    "    print(f\"  Logic Type: {cot_r.get('logic_type', 'N/A')}\")\n",
    "    print(f\"  Depth: {cot_r.get('depth', 'N/A')}\")\n",
    "    if 'context' in cot_r:\n",
    "        print(f\"  Context: {cot_r['context'][:150]}...\")\n",
    "    if 'question' in cot_r:\n",
    "        print(f\"  Question: {cot_r['question'][:150]}...\")\n",
    "    \n",
    "    truth = cot_r.get('ground_truth', cot_r.get('label', 'N/A'))\n",
    "    cot_pred = cot_r.get('prediction', cot_r.get('predicted_label', 'N/A'))\n",
    "    lean_pred = lean_r.get('prediction', lean_r.get('predicted_label', 'N/A'))\n",
    "    \n",
    "    print(f\"  Ground Truth: {truth}\")\n",
    "    print(f\"  CoT:  {cot_pred} ✓\")\n",
    "    print(f\"  Lean: {lean_pred} ✗\")\n",
    "\n",
    "if len(mlogi_cot_only) > 5:\n",
    "    print(f\"\\n... and {len(mlogi_cot_only) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Multi-LogiEval examples where Lean succeeds but CoT fails\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"MULTI-LOGIEVAL: {len(mlogi_lean_only)} QUESTIONS WHERE LEAN SUCCEEDS BUT COT FAILS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, item in enumerate(mlogi_lean_only[:5], 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    cot_r = item['cot']\n",
    "    lean_r = item['lean']\n",
    "    \n",
    "    print(f\"  Logic Type: {cot_r.get('logic_type', 'N/A')}\")\n",
    "    print(f\"  Depth: {cot_r.get('depth', 'N/A')}\")\n",
    "    if 'context' in cot_r:\n",
    "        print(f\"  Context: {cot_r['context'][:150]}...\")\n",
    "    if 'question' in cot_r:\n",
    "        print(f\"  Question: {cot_r['question'][:150]}...\")\n",
    "    \n",
    "    truth = cot_r.get('ground_truth', cot_r.get('label', 'N/A'))\n",
    "    cot_pred = cot_r.get('prediction', cot_r.get('predicted_label', 'N/A'))\n",
    "    lean_pred = lean_r.get('prediction', lean_r.get('predicted_label', 'N/A'))\n",
    "    \n",
    "    print(f\"  Ground Truth: {truth}\")\n",
    "    print(f\"  CoT:  {cot_pred} ✗\")\n",
    "    print(f\"  Lean: {lean_pred} ✓\")\n",
    "\n",
    "if len(mlogi_lean_only) > 5:\n",
    "    print(f\"\\n... and {len(mlogi_lean_only) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Multi-LogiEval results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(['CoT', 'Lean'], \n",
    "        [mlogi_cot_acc, mlogi_lean_acc],\n",
    "        color=['#3498db', '#e74c3c'])\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Multi-LogiEval: Accuracy Comparison', fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "\n",
    "# Performance patterns\n",
    "labels = ['Both\\nCorrect', 'Both\\nWrong', 'CoT Only', 'Lean Only']\n",
    "sizes = [len(mlogi_both_correct), len(mlogi_both_wrong), \n",
    "         len(mlogi_cot_only), len(mlogi_lean_only)]\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12']\n",
    "\n",
    "ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', startangle=90)\n",
    "ax2.set_title('Multi-LogiEval: Performance Patterns', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export findings\nos.makedirs('results/analysis', exist_ok=True)\n\n# FOLIO exports\nwith open('results/analysis/folio_cot_succeeds_lean_fails.json', 'w') as f:\n    json.dump([item['cot'] for item in folio_cot_only], f, indent=2)\n\nwith open('results/analysis/folio_lean_succeeds_cot_fails.json', 'w') as f:\n    json.dump([item['cot'] for item in folio_lean_only], f, indent=2)\n\n# Multi-LogiEval exports\nwith open('results/analysis/mlogi_cot_succeeds_lean_fails.json', 'w') as f:\n    json.dump([item['cot'] for item in mlogi_cot_only], f, indent=2)\n\nwith open('results/analysis/mlogi_lean_succeeds_cot_fails.json', 'w') as f:\n    json.dump([item['cot'] for item in mlogi_lean_only], f, indent=2)\n\n# Summary\nsummary = {\n    'folio': {\n        'cot_acc': folio_cot_acc,\n        'lean_acc': folio_lean_acc,\n        'lean_int_acc': folio_lean_int_acc,\n        'both_correct': len(folio_both_correct),\n        'both_wrong': len(folio_both_wrong),\n        'cot_only': len(folio_cot_only),\n        'lean_only': len(folio_lean_only)\n    },\n    'multilogi': {\n        'cot_acc': mlogi_cot_acc,\n        'lean_acc': mlogi_lean_acc,\n        'both_correct': len(mlogi_both_correct),\n        'both_wrong': len(mlogi_both_wrong),\n        'cot_only': len(mlogi_cot_only),\n        'lean_only': len(mlogi_lean_only)\n    }\n}\n\nwith open('results/analysis/summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"✓ Exported all analysis results to results/analysis/\")\nprint(f\"  - folio_cot_succeeds_lean_fails.json ({len(folio_cot_only)} questions)\")\nprint(f\"  - folio_lean_succeeds_cot_fails.json ({len(folio_lean_only)} questions)\")\nprint(f\"  - mlogi_cot_succeeds_lean_fails.json ({len(mlogi_cot_only)} questions)\")\nprint(f\"  - mlogi_lean_succeeds_cot_fails.json ({len(mlogi_lean_only)} questions)\")\nprint(f\"  - summary.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"FOLIO Dataset:\")\n",
    "print(f\"  - CoT accuracy: {folio_cot_acc:.1f}%\")\n",
    "print(f\"  - Lean accuracy: {folio_lean_acc:.1f}%\")\n",
    "print(f\"  - Questions where CoT succeeds but Lean fails: {len(folio_cot_only)}\")\n",
    "print(f\"  - Questions where Lean succeeds but CoT fails: {len(folio_lean_only)}\")\n",
    "print()\n",
    "print(\"Multi-LogiEval Dataset:\")\n",
    "print(f\"  - CoT accuracy: {mlogi_cot_acc:.1f}%\")\n",
    "print(f\"  - Lean accuracy: {mlogi_lean_acc:.1f}%\")\n",
    "print(f\"  - Questions where CoT succeeds but Lean fails: {len(mlogi_cot_only)}\")\n",
    "print(f\"  - Questions where Lean succeeds but CoT fails: {len(mlogi_lean_only)}\")\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}