{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# DeepSeek-R1 False Positives Analysis\n",
    "\n",
    "False Positive = Lean verification passed but answer is wrong\n",
    "\n",
    "Excluding stories 368 & 435 (cases 75, 76, 77, 156, 157, 158, 159) due to contradictory premises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "exclude_cases = {75, 76, 77, 156, 157, 158, 159}\n",
    "\n",
    "def load_results(path):\n",
    "    with open(path) as f:\n",
    "        return [json.loads(l) for l in f]\n",
    "\n",
    "baseline = load_results('../results/simplelean/deepseek-r1_folio_baseline/results.jsonl')\n",
    "bidir_true = load_results('../results/simplelean/deepseek-r1_folio_bidir_true/results.jsonl')\n",
    "bidir_false = load_results('../results/simplelean/deepseek-r1_folio_bidir_false/results.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(results, name, exclude=exclude_cases):\n",
    "    filtered = [r for r in results if r['case_idx'] not in exclude]\n",
    "    \n",
    "    total = len(filtered)\n",
    "    correct = sum(1 for r in filtered if r['correct'])\n",
    "    lean_pass = sum(1 for r in filtered if r.get('lean_verification') and r['lean_verification'].get('success', False))\n",
    "    \n",
    "    # False positives: Lean pass AND wrong answer\n",
    "    fp = []\n",
    "    for r in filtered:\n",
    "        lean_ok = r.get('lean_verification') and r['lean_verification'].get('success', False)\n",
    "        if lean_ok and not r['correct']:\n",
    "            fp.append({\n",
    "                'case': r['case_idx'],\n",
    "                'pred': r['prediction'],\n",
    "                'gt': r['ground_truth']\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'total': total,\n",
    "        'accuracy': f\"{correct}/{total} ({100*correct/total:.1f}%)\",\n",
    "        'lean_pass': f\"{lean_pass}/{total} ({100*lean_pass/total:.1f}%)\",\n",
    "        'false_positives': len(fp),\n",
    "        'fp_rate': f\"{100*len(fp)/total:.1f}%\",\n",
    "        'fp_details': fp\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    analyze(baseline, 'Baseline'),\n",
    "    analyze(bidir_true, 'bidir_true'),\n",
    "    analyze(bidir_false, 'bidir_false')\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Condition': r['name'],\n",
    "    'Accuracy': r['accuracy'],\n",
    "    'Lean Pass': r['lean_pass'],\n",
    "    'False Positives': r['false_positives'],\n",
    "    'FP Rate': r['fp_rate']\n",
    "} for r in results])\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## False Positives by Error Type\n",
    "\n",
    "- **Gaming**: Proved wrong answer (True when gt=False/Uncertain, or False when gt=True/Uncertain)\n",
    "- **Conservative**: Said Failure/Uncertain when could have proved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_fps(fp_details, condition):\n",
    "    gaming = []\n",
    "    conservative = []\n",
    "    \n",
    "    for fp in fp_details:\n",
    "        pred, gt = fp['pred'], fp['gt']\n",
    "        \n",
    "        if condition == 'Baseline':\n",
    "            if pred == 'True' and gt != 'True':\n",
    "                gaming.append(fp)\n",
    "            else:\n",
    "                conservative.append(fp)\n",
    "        elif condition == 'bidir_true':\n",
    "            if pred == 'True':\n",
    "                gaming.append(fp)\n",
    "            else:  # Failure when gt=True\n",
    "                conservative.append(fp)\n",
    "        elif condition == 'bidir_false':\n",
    "            if pred == 'False':\n",
    "                gaming.append(fp)\n",
    "            else:  # Failure when gt=False\n",
    "                conservative.append(fp)\n",
    "    \n",
    "    return gaming, conservative\n",
    "\n",
    "for r in results:\n",
    "    gaming, conservative = categorize_fps(r['fp_details'], r['name'])\n",
    "    print(f\"\\n{r['name']}:\")\n",
    "    print(f\"  Gaming: {len(gaming)}\")\n",
    "    print(f\"  Conservative: {len(conservative)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Detailed False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE FALSE POSITIVES\")\n",
    "print(\"=\" * 60)\n",
    "for fp in sorted(results[0]['fp_details'], key=lambda x: x['case']):\n",
    "    error_type = \"GAMING\" if fp['pred'] == 'True' and fp['gt'] != 'True' else \"CONSERVATIVE\"\n",
    "    print(f\"Case {fp['case']:3d}: {fp['pred']:>10} -> {fp['gt']:<10} [{error_type}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BIDIR_TRUE FALSE POSITIVES\")\n",
    "print(\"=\" * 60)\n",
    "for fp in sorted(results[1]['fp_details'], key=lambda x: x['case']):\n",
    "    error_type = \"GAMING\" if fp['pred'] == 'True' else \"CONSERVATIVE\"\n",
    "    print(f\"Case {fp['case']:3d}: {fp['pred']:>10} -> {fp['gt']:<10} [{error_type}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BIDIR_FALSE FALSE POSITIVES\")\n",
    "print(\"=\" * 60)\n",
    "for fp in sorted(results[2]['fp_details'], key=lambda x: x['case']):\n",
    "    error_type = \"GAMING\" if fp['pred'] == 'False' else \"CONSERVATIVE\"\n",
    "    print(f\"Case {fp['case']:3d}: {fp['pred']:>10} -> {fp['gt']:<10} [{error_type}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Gaming Cases Comparison\n",
    "\n",
    "Cases where model \"proved\" wrong answer across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find gaming cases for each condition\n",
    "baseline_gaming = {fp['case'] for fp in results[0]['fp_details'] if fp['pred'] == 'True' and fp['gt'] != 'True'}\n",
    "bidir_true_gaming = {fp['case'] for fp in results[1]['fp_details'] if fp['pred'] == 'True'}\n",
    "bidir_false_gaming = {fp['case'] for fp in results[2]['fp_details'] if fp['pred'] == 'False'}\n",
    "\n",
    "print(\"Gaming cases:\")\n",
    "print(f\"  Baseline:    {sorted(baseline_gaming)}\")\n",
    "print(f\"  bidir_true:  {sorted(bidir_true_gaming)}\")\n",
    "print(f\"  bidir_false: {sorted(bidir_false_gaming)}\")\n",
    "print()\n",
    "print(f\"Overlap (baseline & bidir_true): {sorted(baseline_gaming & bidir_true_gaming)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FOLIO data for ground truth\n",
    "with open('../data/folio/original/folio-validation.json') as f:\n",
    "    folio_data = json.load(f)\n",
    "\n",
    "# Gaming cases from bidir_true\n",
    "gaming_cases = [41, 70, 83, 89, 202]\n",
    "\n",
    "# Load as dict for easy lookup\n",
    "baseline_dict = {r['case_idx']: r for r in baseline}\n",
    "bidir_true_dict = {r['case_idx']: r for r in bidir_true}\n",
    "bidir_false_dict = {r['case_idx']: r for r in bidir_false}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GAMING CASES COMPARISON (bidir_true gaming cases)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Case':<6} {'GT':<12} {'Baseline':<12} {'bidir_true':<12} {'bidir_false':<12} {'Story'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx in gaming_cases:\n",
    "    gt = folio_data[idx]['label']\n",
    "    story = folio_data[idx].get('story_id', '?')\n",
    "    \n",
    "    bl = baseline_dict[idx]['prediction']\n",
    "    bt = bidir_true_dict[idx]['prediction']\n",
    "    bf = bidir_false_dict[idx]['prediction']\n",
    "    \n",
    "    bl_mark = \"ok\" if baseline_dict[idx]['correct'] else \"X\"\n",
    "    bt_mark = \"ok\" if bidir_true_dict[idx]['correct'] else \"X\"\n",
    "    bf_mark = \"ok\" if bidir_false_dict[idx]['correct'] else \"X\"\n",
    "    \n",
    "    print(f\"{idx:<6} {gt:<12} {bl+' '+bl_mark:<12} {bt+' '+bt_mark:<12} {bf+' '+bf_mark:<12} {story}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Gaming Cases Analysis\n",
    "\n",
    "Analyzing each gaming case to determine if it's true gaming or dataset issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def show_case(idx):\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"CASE {idx}: PROBLEM\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Premises:\\n{folio_data[idx]['premises']}\\n\")\n",
    "    print(f\"Conclusion: {folio_data[idx]['conclusion']}\")\n",
    "    print(f\"Ground Truth: {folio_data[idx]['label']}\")\n",
    "\n",
    "def show_lean(idx, condition='bidir_true'):\n",
    "    path = f'../results/simplelean/deepseek-r1_folio_{condition}/responses/case_{idx}.txt'\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            content = f.read()\n",
    "        lean_match = re.search(r'<lean>(.*?)</lean>', content, re.DOTALL)\n",
    "        if not lean_match:\n",
    "            lean_match = re.search(r'```lean4?\\n(.*?)```', content, re.DOTALL)\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"CASE {idx}: {condition} LEAN CODE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(lean_match.group(1).strip() if lean_match else \"No Lean code found\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Case 41 (GT: False, Model: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_case(41)\n",
    "show_lean(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Case 70 (GT: Uncertain, Model: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_case(70)\n",
    "show_lean(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Case 83 (GT: False, Model: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_case(83)\n",
    "show_lean(83)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Case 89 (GT: Uncertain, Model: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_case(89)\n",
    "show_lean(89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Case 202 (GT: Uncertain, Model: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_case(202)\n",
    "show_lean(202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "## Summary: Gaming Cases Analysis\n\n**All gaming cases across conditions:**\n\n| Case | GT | GPT-5 BL | GPT-5 BT | DS-R1 BL | DS-R1 BT | Issue | Verdict |\n|------|-----|----------|----------|----------|----------|-------|---------|\n| 41 | False | GAME | ok | GAME | GAME | Valid proof via contradiction | **Model correct** |\n| 70 | Uncertain | GAME | GAME | ok | GAME | Invented axiom (Stock subset) | **GAMING** |\n| 83 | False | GAME | GAME | GAME | GAME | Vacuous truth | **Debatable** |\n| 89 | Uncertain | GAME | GAME | GAME | GAME | Valid chain reasoning | **Model correct** |\n| 202 | Uncertain | GAME | GAME | GAME | GAME | Valid inference | **Model correct** |\n\n**Observations:**\n- GPT-5 bidir_true fixed Case 41 (5→4 gaming)\n- DeepSeek-R1 baseline got Case 70 correct, but bidir_true introduced it (4→5)\n- **bidir_false: 0 gaming for both models**\n\n### Conclusion\n- **1 true gaming case**: Case 70 (invented axiom)\n- **1 debatable case**: Case 83 (vacuous truth - classical logic says True)\n- **3 dataset issues**: Cases 41, 89, 202 (model's reasoning appears valid)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}