{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SimpleLean Baseline Analysis\n\nAnalyzing GPT-5 baseline results on FOLIO dataset (203 cases)."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load baseline results\nresults_path = Path('../results/simplelean/gpt-5_folio_baseline_20251223_000248/results.jsonl')\nresults = [json.loads(l) for l in results_path.read_text().strip().split('\\n')]\nprint(f'Loaded {len(results)} results')\n\n# Quick summary\ncorrect = sum(1 for r in results if r.get('correct'))\nlean_pass = sum(1 for r in results if (r.get('lean_verification') or {}).get('success'))\nprint(f'Accuracy: {correct}/{len(results)} ({100*correct/len(results):.1f}%)')\nprint(f'Lean Pass: {lean_pass}/{len(results)} ({100*lean_pass/len(results):.1f}%)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Error analysis by ground truth\nfrom collections import defaultdict\n\ndef normalize(ans):\n    return 'Uncertain' if ans == 'Unknown' else ans\n\n# Group by ground truth\nby_gt = defaultdict(list)\nfor r in results:\n    by_gt[r.get('ground_truth')].append(r)\n\nprint('Accuracy by Ground Truth:')\nprint('=' * 50)\nfor gt in ['True', 'False', 'Uncertain']:\n    cases = by_gt[gt]\n    correct = sum(1 for r in cases if normalize(r.get('prediction')) == gt)\n    print(f'{gt:12} {correct}/{len(cases)} ({100*correct/len(cases):.1f}%)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Error patterns (GT → Prediction)\nerror_patterns = Counter()\nfor r in results:\n    gt = r.get('ground_truth')\n    pred = normalize(r.get('prediction'))\n    if gt != pred:\n        error_patterns[f'{gt} → {pred}'] += 1\n\nprint('Error Patterns:')\nprint('=' * 50)\nfor pattern, count in error_patterns.most_common():\n    print(f'{pattern:25} {count} cases')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gaming detection: Lean passed but wrong definitive answer\ngaming_cases = []\nfor r in results:\n    gt = r.get('ground_truth')\n    pred = normalize(r.get('prediction'))\n    lean_pass = (r.get('lean_verification') or {}).get('success')\n    \n    # Gaming: proved opposite (True→False or False→True)\n    if lean_pass and ((gt == 'True' and pred == 'False') or (gt == 'False' and pred == 'True')):\n        gaming_cases.append(r)\n\nprint(f'Gaming cases (Lean passed but proved opposite): {len(gaming_cases)}')\nfor r in gaming_cases:\n    print(f\"  Case {r.get('case_idx')}: {r.get('ground_truth')} → {normalize(r.get('prediction'))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# False negatives: Lean passed but wrong answer (conservative errors)\nfalse_neg_uncertain = []  # Said Uncertain when should have proved something\nfor r in results:\n    gt = r.get('ground_truth')\n    pred = normalize(r.get('prediction'))\n    lean_pass = (r.get('lean_verification') or {}).get('success')\n    \n    if lean_pass and pred == 'Uncertain' and gt in ['True', 'False']:\n        false_neg_uncertain.append(r)\n\nprint(f'Conservative errors (Uncertain when GT was True/False): {len(false_neg_uncertain)}')\nby_gt = Counter(r.get('ground_truth') for r in false_neg_uncertain)\nfor gt, count in by_gt.most_common():\n    print(f'  {gt} → Uncertain: {count} cases')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Iteration distribution\niters = Counter(r.get('num_iterations', 0) for r in results)\nprint('Iteration distribution:')\nfor i in sorted(iters.keys()):\n    pct = 100 * iters[i] / len(results)\n    print(f'  {i} iter: {iters[i]} cases ({pct:.1f}%)')\n\n# Success rate by iteration count\nprint('\\nSuccess rate by iteration:')\nfor i in sorted(iters.keys()):\n    cases = [r for r in results if r.get('num_iterations') == i]\n    correct = sum(1 for r in cases if r.get('correct'))\n    print(f'  {i} iter: {correct}/{len(cases)} ({100*correct/len(cases):.1f}%)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cases that failed Lean verification\nlean_failed = [r for r in results if not (r.get('lean_verification') or {}).get('success')]\nprint(f'Lean verification failed: {len(lean_failed)} cases')\n\n# Show error types\nerror_types = Counter()\nfor r in lean_failed:\n    lv = r.get('lean_verification') or {}\n    errors = lv.get('errors', [])\n    if errors:\n        first_error = errors[0].lower()[:100]\n        if 'unknown identifier' in first_error:\n            error_types['unknown_identifier'] += 1\n        elif 'type mismatch' in first_error:\n            error_types['type_mismatch'] += 1\n        elif 'unexpected' in first_error:\n            error_types['syntax_error'] += 1\n        else:\n            error_types['other'] += 1\n    else:\n        error_types['no_code'] += 1\n\nprint('\\nError types:')\nfor err, count in error_types.most_common():\n    print(f'  {err}: {count}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample failed cases\nfailed = [r for r in results if not r.get('correct')]\nprint(f'Total failed cases: {len(failed)}')\n\n# Show a few examples\nfor r in failed[:3]:\n    print(f\"\\nCase {r.get('case_idx')}: {r.get('ground_truth')} → {normalize(r.get('prediction'))}\")\n    print(f\"  Lean Pass: {(r.get('lean_verification') or {}).get('success')}\")\n    print(f\"  Iterations: {r.get('num_iterations')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary statistics\nprint('=' * 60)\nprint('BASELINE RESULTS SUMMARY')\nprint('=' * 60)\nprint(f'Total cases: {len(results)}')\nprint(f'Accuracy: {correct}/{len(results)} ({100*correct/len(results):.1f}%)')\nprint(f'Lean Pass: {lean_pass}/{len(results)} ({100*lean_pass/len(results):.1f}%)')\nprint()\nprint('Errors breakdown:')\nprint(f'  Gaming (proved opposite): {len(gaming_cases)}')\nprint(f'  Conservative (Uncertain when T/F): {len(false_neg_uncertain)}')\nprint(f'  Lean failed: {len(lean_failed)}')"
  },
  {
   "cell_type": "markdown",
   "source": "# Save failed case indices for retry experiments\nfailed_indices = [r.get('case_idx') for r in failed]\nprint(f'Failed case indices ({len(failed_indices)} cases):')\nprint(failed_indices)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}