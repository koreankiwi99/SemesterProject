{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleLean Failure Analysis\n",
    "\n",
    "Analyzing cases that failed after 3 iterations (max retries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 91 results\n"
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "results_path = Path('../results/simplelean/gpt-5_folio_implicit_20251222_215452/results.jsonl')\n",
    "results = [json.loads(l) for l in results_path.read_text().strip().split('\\n')]\n",
    "print(f'Loaded {len(results)} results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration distribution:\n",
      "  1 iter: 8 cases\n",
      "  2 iter: 28 cases\n",
      "  3 iter: 55 cases\n"
     ]
    }
   ],
   "source": [
    "# Iteration distribution\n",
    "iters = Counter(r.get('num_iterations', 0) for r in results)\n",
    "print('Iteration distribution:')\n",
    "for i in sorted(iters.keys()):\n",
    "    print(f'  {i} iter: {iters[i]} cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases with 3 iterations: 55\n"
     ]
    }
   ],
   "source": [
    "# Filter 3-iteration cases (max retries used)\n",
    "failed_cases = [r for r in results if r.get('num_iterations') == 3]\n",
    "print(f'Cases with 3 iterations: {len(failed_cases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lean 3 syntax patterns to detect\n",
    "lean3_patterns = [\n",
    "    (r'\\bconstant\\b', 'constant (use opaque/axiom)'),\n",
    "    (r'λ\\s*\\w+\\s*,', 'λ x, (use fun x =>)'),\n",
    "    (r'\\bbegin\\b', 'begin...end (use by)'),\n",
    "    (r'#check\\b', '#check'),\n",
    "    (r'\\blemma\\b', 'lemma (use theorem)'),\n",
    "    (r'by\\s*\\{', 'by { (use by with newline)'),\n",
    "    (r'\\bvariable\\b(?!s)', 'variable (check context)'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lean 3 syntax detected: 37\n",
      "Lean 4 but errors: 17\n",
      "No Lean code: 1\n"
     ]
    }
   ],
   "source": [
    "# Detect Lean 3 syntax in failed cases\n",
    "lean3_cases = []\n",
    "lean4_error_cases = []\n",
    "no_code_cases = []\n",
    "\n",
    "for r in failed_cases:\n",
    "    lean_code = r.get('lean_code') or ''\n",
    "    case_idx = r.get('case_idx')\n",
    "    \n",
    "    if not lean_code:\n",
    "        no_code_cases.append(r)\n",
    "        continue\n",
    "    \n",
    "    found_patterns = []\n",
    "    for pattern, name in lean3_patterns:\n",
    "        if re.search(pattern, lean_code):\n",
    "            found_patterns.append(name)\n",
    "    \n",
    "    if found_patterns:\n",
    "        lean3_cases.append({'case': r, 'patterns': found_patterns})\n",
    "    else:\n",
    "        lean4_error_cases.append(r)\n",
    "\n",
    "print(f'Lean 3 syntax detected: {len(lean3_cases)}')\n",
    "print(f'Lean 4 but errors: {len(lean4_error_cases)}')\n",
    "print(f'No Lean code: {len(no_code_cases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cases with Lean 3 syntax ===\n",
      "\n",
      "Case 35: constant (use opaque/axiom)\n",
      "  Prediction: False, GT: False\n",
      "  Code preview: constant Entity : Type\n",
      "\n",
      "constant IsStable : Entity → Prop\n",
      "constant Includes : Entity → Entity → Prop\n",
      "constant Feud : Entity → Entity → Prop\n",
      "\n",
      "constant DiamondMine : Entity\n",
      "constant Imperium : Entity\n",
      "co...\n",
      "\n",
      "Case 106: variable (check context)\n",
      "  Prediction: False, GT: False\n",
      "  Code preview: universe u\n",
      "\n",
      "section\n",
      "\n",
      "-- Types\n",
      "variable (Person Entity : Type u)\n",
      "\n",
      "-- Distinguished entities/persons\n",
      "variable (ETS : Entity)\n",
      "variable (Tom : Person)\n",
      "\n",
      "-- Predicates\n",
      "variable (Applicant TakingGRE LivesSin...\n",
      "\n",
      "Case 108: constant (use opaque/axiom)\n",
      "  Prediction: False, GT: False\n",
      "  Code preview: constant Person : Type\n",
      "constant Spill : Person → Prop\n",
      "constant Tidy : Person → Prop\n",
      "constant Clean : Person → Prop\n",
      "constant ValueOrder : Person → Prop\n",
      "constant FamilyPrior : Person → Prop\n",
      "constant Clu...\n",
      "\n",
      "Case 38: constant (use opaque/axiom)\n",
      "  Prediction: False, GT: False\n",
      "  Code preview: constant Person : Type\n",
      "constant Organization : Type\n",
      "constant Work : Type\n",
      "\n",
      "constant Beethoven : Person\n",
      "constant ViennaMusicSociety : Organization\n",
      "constant SymphonyNo9 : Work\n",
      "\n",
      "constant MusicPiece : Work...\n",
      "\n",
      "Case 111: constant (use opaque/axiom)\n",
      "  Prediction: True, GT: True\n",
      "  Code preview: -- Fixed Lean 4 code without universe issues\n",
      "\n",
      "constant Entity : Type\n",
      "\n",
      "-- Predicates and relations\n",
      "constant MountainRange : Entity → Prop\n",
      "constant InNM : Entity → Prop\n",
      "constant InTX : Entity → Prop\n",
      "con...\n"
     ]
    }
   ],
   "source": [
    "# Show Lean 3 cases\n",
    "print('=== Cases with Lean 3 syntax ===')\n",
    "for item in lean3_cases[:5]:\n",
    "    r = item['case']\n",
    "    print(f\"\\nCase {r.get('case_idx')}: {', '.join(item['patterns'])}\")\n",
    "    print(f\"  Prediction: {r.get('prediction')}, GT: {r.get('ground_truth')}\")\n",
    "    print(f\"  Code preview: {(r.get('lean_code') or '')[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cases with Lean 4 syntax but errors ===\n",
      "\n",
      "Case 36:\n",
      "  Prediction: Unknown, GT: True\n",
      "  Error: declaration `beethoven_is_conductor` contains universe level metavariables at the expression\n",
      "  orchestras_led_by_conductors.{u_1, ?u.115} Beethoven Vi...\n",
      "\n",
      "Case 185:\n",
      "  Prediction: False, GT: False\n",
      "\n",
      "Case 41:\n",
      "  Prediction: True, GT: False\n",
      "\n",
      "Case 187:\n",
      "  Prediction: Unknown, GT: Uncertain\n",
      "\n",
      "Case 189:\n",
      "  Prediction: False, GT: False\n"
     ]
    }
   ],
   "source": [
    "# Show Lean 4 error cases\n",
    "print('=== Cases with Lean 4 syntax but errors ===')\n",
    "for r in lean4_error_cases[:5]:\n",
    "    lv = r.get('lean_verification') or {}\n",
    "    errors = lv.get('errors', [])\n",
    "    print(f\"\\nCase {r.get('case_idx')}:\")\n",
    "    print(f\"  Prediction: {r.get('prediction')}, GT: {r.get('ground_truth')}\")\n",
    "    if errors:\n",
    "        print(f\"  Error: {errors[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error type distribution:\n",
      "  syntax_error: 32\n",
      "  no_lean_code: 16\n",
      "  other: 6\n",
      "  unknown_identifier: 1\n"
     ]
    }
   ],
   "source": [
    "# Error categorization\n",
    "error_types = Counter()\n",
    "\n",
    "for r in failed_cases:\n",
    "    lv = r.get('lean_verification') or {}\n",
    "    errors = lv.get('errors', [])\n",
    "    \n",
    "    if not errors:\n",
    "        error_types['no_lean_code'] += 1\n",
    "        continue\n",
    "    \n",
    "    first_error = errors[0].lower()\n",
    "    \n",
    "    if 'unknown identifier' in first_error:\n",
    "        error_types['unknown_identifier'] += 1\n",
    "    elif 'type mismatch' in first_error:\n",
    "        error_types['type_mismatch'] += 1\n",
    "    elif 'failed to synthesize' in first_error:\n",
    "        error_types['synthesis_failed'] += 1\n",
    "    elif 'unexpected' in first_error:\n",
    "        error_types['syntax_error'] += 1\n",
    "    elif 'sorry' in first_error:\n",
    "        error_types['uses_sorry'] += 1\n",
    "    else:\n",
    "        error_types['other'] += 1\n",
    "\n",
    "print('Error type distribution:')\n",
    "for err, count in error_types.most_common():\n",
    "    print(f'  {err}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 91\n",
      "Correct: 50/91 (54.9%)\n",
      "Lean Pass: 51/91 (56.0%)\n"
     ]
    }
   ],
   "source": [
    "# Overall stats\n",
    "total = len(results)\n",
    "correct = sum(1 for r in results if r.get('correct'))\n",
    "lean_pass = sum(1 for r in results if (r.get('lean_verification') or {}).get('success'))\n",
    "\n",
    "print(f'Total: {total}')\n",
    "print(f'Correct: {correct}/{total} ({100*correct/total:.1f}%)')\n",
    "print(f'Lean Pass: {lean_pass}/{total} ({100*lean_pass/total:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Prompt Variants Comparison\n\nTesting improved prompts with Lean 4 examples on the 55 cases that failed 3 iterations.\n\n### Prompt Differences:\n- **lean4_specified**: Lean 4 syntax rules + True example only\n- **lean4_balanced**: Lean 4 syntax + True AND Uncertain examples\n- **lean4_minimal**: Same as balanced but WITHOUT \"CRITICAL RULES\" section\n\n### Verified Lean 4 Example (added to prompts):\n\n```lean\n-- Example 1 (True): \"The cat is blue. If someone is blue then they are nice.\"\naxiom obj : Type\naxiom Cat : obj\naxiom Blue : obj → Prop\naxiom Nice : obj → Prop\naxiom T1 : Blue Cat\naxiom R1 : ∀ x : obj, Blue x → Nice x\ntheorem cat_nice : Nice Cat := R1 Cat T1\n\n-- Example 2 (Uncertain): \"The cat is blue. If someone is nice then they are red.\"\naxiom Red : obj → Prop\naxiom R2 : ∀ x : obj, Nice x → Red x\n-- Cannot prove Red Cat or ¬ Red Cat from given axioms\n```\n\nKey syntax differences fixed:\n- `axiom` instead of `constant` (Lean 3)\n- `fun x =>` instead of `λ x,` (Lean 3)\n- `by` with tactics instead of `begin...end` (Lean 3)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load and compare all prompt variants on 55 failed cases\n\ndef load_and_analyze(path):\n    \"\"\"Load results and compute metrics with Unknown→Uncertain normalization.\"\"\"\n    results = [json.loads(l) for l in Path(path).read_text().strip().split('\\n')]\n    \n    def normalize(ans):\n        return 'Uncertain' if ans == 'Unknown' else ans\n    \n    total = len(results)\n    correct = sum(1 for r in results \n                  if normalize(r.get('prediction')) == r.get('ground_truth'))\n    lean_pass = sum(1 for r in results \n                    if (r.get('lean_verification') or {}).get('success'))\n    # False negatives: lean passed but wrong answer\n    false_neg = sum(1 for r in results \n                    if (r.get('lean_verification') or {}).get('success')\n                    and normalize(r.get('prediction')) != r.get('ground_truth'))\n    \n    return {\n        'total': total,\n        'correct': correct,\n        'lean_pass': lean_pass,\n        'false_neg': false_neg,\n        'accuracy': correct / total,\n        'lean_rate': lean_pass / total,\n        'false_neg_rate': false_neg / lean_pass if lean_pass > 0 else 0\n    }\n\n# Prompt variants\nprompts = {\n    'lean4_specified': '../results/simplelean/gpt-5_folio_lean4_specified_20251222_230613/results.jsonl',\n    'lean4_balanced': '../results/simplelean/gpt-5_folio_lean4_balanced_20251222_231818/results.jsonl',\n    'lean4_minimal': '../results/simplelean/gpt-5_folio_lean4_minimal_20251222_233118/results.jsonl',\n}\n\nprint('=' * 70)\nprint('PROMPT COMPARISON ON 55 FAILED CASES')\nprint('=' * 70)\nprint(f'{\"Prompt\":<20} {\"Accuracy\":<15} {\"Lean Pass\":<15} {\"False Neg Rate\"}')\nprint('-' * 70)\n\nfor name, path in prompts.items():\n    m = load_and_analyze(path)\n    print(f'{name:<20} {m[\"correct\"]}/{m[\"total\"]} ({m[\"accuracy\"]*100:.1f}%)'\n          f'    {m[\"lean_pass\"]}/{m[\"total\"]} ({m[\"lean_rate\"]*100:.1f}%)'\n          f'    {m[\"false_neg\"]}/{m[\"lean_pass\"]} ({m[\"false_neg_rate\"]*100:.1f}%)')\n\nprint('=' * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}