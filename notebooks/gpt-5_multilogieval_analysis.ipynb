{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GPT-5 Multi-LogiEval Analysis\n",
    "\n",
    "**Definitions:**\n",
    "- **False Positive** = Lean verification passed but answer is wrong\n",
    "- **Gaming** = False positive where model proved Yes/No (not Uncertain)\n",
    "- **Conservative** = False positive where model said Uncertain when should have proved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load results\n",
    "with open('../results/simplelean/gpt-5_multilogieval_baseline_20251224_190323/results.jsonl') as f:\n",
    "    results = [json.loads(l) for l in f]\n",
    "\n",
    "# Load original data\n",
    "with open('../data/multilogieval/multilogieval-sampled.json') as f:\n",
    "    data = {d['idx']: d for d in json.load(f)}\n",
    "\n",
    "print(f\"Loaded {len(results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by depth and GT\n",
    "stats = defaultdict(lambda: defaultdict(lambda: {'total': 0, 'correct': 0, 'fp': 0, 'gaming': 0}))\n",
    "\n",
    "for r in results:\n",
    "    depth, gt = r['depth'], r['ground_truth']\n",
    "    stats[depth][gt]['total'] += 1\n",
    "    \n",
    "    lean_ok = r.get('lean_verification', {}).get('success', False)\n",
    "    if r['correct']:\n",
    "        stats[depth][gt]['correct'] += 1\n",
    "    elif lean_ok:\n",
    "        stats[depth][gt]['fp'] += 1\n",
    "        if r['prediction'] and r['prediction'].lower() in ['yes', 'no']:\n",
    "            stats[depth][gt]['gaming'] += 1\n",
    "\n",
    "# Display\n",
    "rows = []\n",
    "for depth in ['d3', 'd4', 'd5']:\n",
    "    for gt in ['yes', 'no']:\n",
    "        s = stats[depth][gt]\n",
    "        if s['total'] > 0:\n",
    "            rows.append({\n",
    "                'Depth': depth,\n",
    "                'GT': gt,\n",
    "                'Acc': f\"{s['correct']}/{s['total']} ({100*s['correct']/s['total']:.1f}%)\",\n",
    "                'FP': s['fp'],\n",
    "                'Gaming': s['gaming']\n",
    "            })\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Gaming Case: Case 67\n",
    "\n",
    "The only gaming case (pred=Yes, gt=No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find case 67\n",
    "case_67 = next(r for r in results if r['case_idx'] == 67)\n",
    "orig_67 = data[67]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CASE 67\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Depth: {case_67['depth']}\")\n",
    "print(f\"Rule: {case_67['rule']}\")\n",
    "print(f\"Ground Truth: {case_67['ground_truth']}\")\n",
    "print(f\"Prediction: {case_67['prediction']}\")\n",
    "print(f\"Lean Pass: {case_67['lean_verification']['success']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONTEXT\")\n",
    "print(\"=\" * 70)\n",
    "print(orig_67['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUESTION\")\n",
    "print(\"=\" * 70)\n",
    "print(orig_67['question'])\n",
    "print(f\"\\nAnswer: {orig_67['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LEAN CODE\")\n",
    "print(\"=\" * 70)\n",
    "print(case_67['lean_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## Analysis of Case 67\n\n**Question:** \"The kitchen did not get hot, did Sam eat dinner last night?\"\n**GT:** No | **Pred:** Yes\n\n**Model's reasoning (valid!):**\n1. Kitchen not hot → Stove not on (contrapositive of R1: StoveOn → KitchenHot)\n2. R3: StoveOn ∨ ¬UseStove → with ¬StoveOn → ¬UseStove\n3. If Sam didn't eat dinner → hungry (R5) → makes pancakes (R4) → uses stove (R2)\n4. But ¬UseStove, so by contradiction: Sam must have eaten dinner\n\n**Verdict: MODEL CORRECT (Dataset issue)**\n\nThe proof is logically valid. If we accept the premises, Sam eating dinner is the only consistent scenario when the kitchen didn't get hot."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load response file for full context\n",
    "with open('../results/simplelean/gpt-5_multilogieval_baseline_20251224_190323/responses/case_67.txt') as f:\n",
    "    response = f.read()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FULL RESPONSE\")\n",
    "print(\"=\" * 70)\n",
    "print(response[:3000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}