#!/usr/bin/env python3
"""
Test memorization using pre-generated perturbations.

This script loads perturbations generated by generate_perturbations.py
and tests them on a target model (e.g., gpt-5).
"""

import argparse
import asyncio
import json
import os
from datetime import datetime
from tqdm.asyncio import tqdm_asyncio
from openai import AsyncOpenAI

from utils.answer_parsing import parse_folio_answer, parse_multilogieval_answer, normalize_answer


async def test_folio_with_perturbation(pert_data, client, model, semaphore):
    """Test a FOLIO example with its pre-generated perturbations."""
    async with semaphore:
        conclusion = pert_data['conclusion']
        ground_truth = normalize_answer(pert_data['ground_truth'], answer_format="true_false")
        original_premises = pert_data['original_premises']

        system_prompt = "You are a logical reasoning assistant. Determine if conclusions follow from premises."

        results = {
            'example_id': pert_data.get('example_id'),
            'story_id': pert_data.get('story_id'),
            'conclusion': conclusion,
            'ground_truth': ground_truth,
            'perturbations': {}
        }

        try:
            # Test original
            orig_prompt = f"""Premises:
{chr(10).join('- ' + p for p in original_premises)}

Conclusion: {conclusion}

Based ONLY on the premises, is the conclusion True, False, or Unknown?
Answer with exactly one word: True, False, or Unknown"""

            orig_resp = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": orig_prompt}
                ]
            )
            orig_answer = parse_folio_answer(orig_resp.choices[0].message.content)
            results['orig_answer'] = orig_answer
            results['orig_correct'] = orig_answer == ground_truth

            # Test each perturbation
            for pert_type, pert_info in pert_data['perturbations'].items():
                perturbed_premises = pert_info['perturbed_premises']

                pert_prompt = f"""Premises:
{chr(10).join('- ' + p for p in perturbed_premises)}

Conclusion: {conclusion}

Based ONLY on the premises, is the conclusion True, False, or Unknown?
Answer with exactly one word: True, False, or Unknown"""

                pert_resp = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": pert_prompt}
                    ]
                )
                pert_answer = parse_folio_answer(pert_resp.choices[0].message.content)

                results['perturbations'][pert_type] = {
                    'description': pert_info['description'],
                    'perturbed_answer': pert_answer,
                    'answer_changed': pert_answer != orig_answer,
                    'expected_change': pert_info['expected_answer_change']
                }

            # Compute verdict
            results['verdict'] = compute_verdict(results)
            return results

        except Exception as e:
            return {'example_id': pert_data.get('example_id'), 'error': str(e)}


async def test_multilogi_with_perturbation(pert_data, client, model, semaphore):
    """Test a Multi-LogiEval example with its pre-generated perturbations."""
    async with semaphore:
        question = pert_data['question']
        ground_truth = normalize_answer(pert_data['ground_truth'], answer_format="yes_no")
        original_context = pert_data['original_context']

        system_prompt = "You are a logical reasoning expert. Answer based only on the given context."

        results = {
            'logic_type': pert_data.get('logic_type'),
            'depth': pert_data.get('depth'),
            'rule': pert_data.get('rule'),
            'question': question,
            'ground_truth': ground_truth,
            'perturbations': {}
        }

        try:
            # Test original
            orig_prompt = f"""Context: {original_context}

Question: {question}

Answer with exactly one word: Yes or No"""

            orig_resp = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": orig_prompt}
                ]
            )
            orig_answer = parse_multilogieval_answer(orig_resp.choices[0].message.content)
            results['orig_answer'] = orig_answer
            results['orig_correct'] = orig_answer == ground_truth

            # Test each perturbation
            for pert_type, pert_info in pert_data['perturbations'].items():
                perturbed_context = pert_info['perturbed_context']

                pert_prompt = f"""Context: {perturbed_context}

Question: {question}

Answer with exactly one word: Yes or No"""

                pert_resp = await client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": pert_prompt}
                    ]
                )
                pert_answer = parse_multilogieval_answer(pert_resp.choices[0].message.content)

                results['perturbations'][pert_type] = {
                    'description': pert_info['description'],
                    'perturbed_answer': pert_answer,
                    'answer_changed': pert_answer != orig_answer,
                    'expected_change': pert_info['expected_answer_change']
                }

            results['verdict'] = compute_verdict(results)
            return results

        except Exception as e:
            return {'error': str(e)}


def compute_verdict(result):
    """Compute verdict based on perturbation responses.

    Verdicts:
    - LIKELY_MEMORIZED: Both perturbations show no change when change expected
    - POSSIBLY_MEMORIZED: One perturbation shows no change
    - ROBUST: Answer changes appropriately with perturbations
    - ORIGINALLY_WRONG: Model got original wrong, can't assess memorization
    """
    if not result.get('orig_correct', False):
        return 'ORIGINALLY_WRONG'

    perts = result.get('perturbations', {})
    if not perts:
        return 'INCONCLUSIVE'

    unchanged_count = 0
    total_perts = 0

    for pert_name, pert_data in perts.items():
        if pert_data.get('expected_change', False):
            total_perts += 1
            if not pert_data.get('answer_changed', False):
                unchanged_count += 1

    if total_perts == 0:
        return 'INCONCLUSIVE'

    if unchanged_count == total_perts:
        return 'LIKELY_MEMORIZED'
    elif unchanged_count > 0:
        return 'POSSIBLY_MEMORIZED'
    else:
        return 'ROBUST'


async def main():
    parser = argparse.ArgumentParser(description='Test memorization with pre-generated perturbations')
    parser.add_argument('--api_key', required=True, help='OpenAI API key')
    parser.add_argument('--perturbations_file', required=True,
                        help='Path to perturbations JSON file from generate_perturbations.py')
    parser.add_argument('--model', default='gpt-5', help='Model to test')
    parser.add_argument('--concurrency', type=int, default=5)
    parser.add_argument('--output_dir', default='results/memorization')

    args = parser.parse_args()

    # Load perturbations
    print(f"Loading perturbations from: {args.perturbations_file}")
    with open(args.perturbations_file) as f:
        pert_data = json.load(f)

    generator_model = pert_data.get('generator_model', 'unknown')
    perturbations = pert_data.get('perturbations', {})

    client = AsyncOpenAI(api_key=args.api_key)
    semaphore = asyncio.Semaphore(args.concurrency)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(args.output_dir, f"test_{args.model}_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)

    print("=" * 70)
    print("MEMORIZATION TEST WITH PRE-GENERATED PERTURBATIONS")
    print("=" * 70)
    print(f"Test Model: {args.model}")
    print(f"Perturbation Generator: {generator_model}")
    print(f"Output: {output_dir}")
    print("=" * 70)

    all_results = {'folio': [], 'multilogieval': []}

    # Test FOLIO
    if 'folio' in perturbations and perturbations['folio']:
        folio_perts = perturbations['folio']
        print(f"\nTesting FOLIO: {len(folio_perts)} examples")

        tasks = [test_folio_with_perturbation(p, client, args.model, semaphore)
                 for p in folio_perts]
        results = await tqdm_asyncio.gather(*tasks, desc="FOLIO")
        all_results['folio'] = [r for r in results if not r.get('error')]

    # Test Multi-LogiEval
    if 'multilogieval' in perturbations and perturbations['multilogieval']:
        multi_perts = perturbations['multilogieval']
        print(f"\nTesting Multi-LogiEval: {len(multi_perts)} examples")

        tasks = [test_multilogi_with_perturbation(p, client, args.model, semaphore)
                 for p in multi_perts]
        results = await tqdm_asyncio.gather(*tasks, desc="MultiLogiEval")
        all_results['multilogieval'] = [r for r in results if not r.get('error')]

    # Compute summary
    print("\n" + "=" * 70)
    print("RESULTS SUMMARY")
    print("=" * 70)

    summary = {}
    for dataset, results in all_results.items():
        if not results:
            continue

        verdicts = {'LIKELY_MEMORIZED': 0, 'POSSIBLY_MEMORIZED': 0, 'ROBUST': 0,
                    'ORIGINALLY_WRONG': 0, 'INCONCLUSIVE': 0}

        for r in results:
            v = r.get('verdict', 'INCONCLUSIVE')
            if v in verdicts:
                verdicts[v] += 1

        total = len(results)
        mem_count = verdicts['LIKELY_MEMORIZED'] + verdicts['POSSIBLY_MEMORIZED']

        summary[dataset] = {
            'total': total,
            'verdicts': verdicts,
            'memorization_rate': mem_count / total * 100 if total > 0 else 0
        }

        print(f"\n{dataset.upper()} (n={total}):")
        for v, count in verdicts.items():
            pct = count/total*100 if total > 0 else 0
            print(f"  {v}: {count} ({pct:.1f}%)")
        print(f"  → Memorization signal: {mem_count}/{total} ({summary[dataset]['memorization_rate']:.1f}%)")

    # Save results
    output_file = os.path.join(output_dir, 'results.json')
    with open(output_file, 'w') as f:
        json.dump({
            'test_model': args.model,
            'generator_model': generator_model,
            'perturbations_file': args.perturbations_file,
            'timestamp': timestamp,
            'summary': summary,
            'folio_results': all_results['folio'],
            'multilogieval_results': all_results['multilogieval']
        }, f, indent=2)

    print(f"\nResults saved to: {output_file}")

    # Overall interpretation
    total_mem = sum(s.get('verdicts', {}).get('LIKELY_MEMORIZED', 0) +
                   s.get('verdicts', {}).get('POSSIBLY_MEMORIZED', 0)
                   for s in summary.values())
    total_all = sum(s.get('total', 0) for s in summary.values())

    if total_all > 0:
        overall_rate = total_mem / total_all * 100
        print(f"\n{'=' * 70}")
        print(f"Overall potential memorization: {total_mem}/{total_all} ({overall_rate:.1f}%)")

        if overall_rate > 40:
            print("⚠️  HIGH MEMORIZATION SIGNAL")
        elif overall_rate > 20:
            print("⚠️  MODERATE MEMORIZATION SIGNAL")
        else:
            print("✓  LOW MEMORIZATION SIGNAL")


if __name__ == "__main__":
    asyncio.run(main())
